#!/usr/bin/env python3
"""Generate Rust code from the n-gram model for word generation."""

import argparse
import json
from pathlib import Path


def escape_rust_string(s: str) -> str:
    """Escape a string for use in Rust code."""
    return s.replace("\\", "\\\\").replace('"', '\\"')


def generate_rust_code(model: dict) -> str:
    """Generate Rust module code from the model."""
    id_to_token = model["id_to_token"]
    ngrams = model["ngrams"]
    end_ngrams = model["end_ngrams"]
    ngram_size = int(model["ngram_size"])
    probability_bits = int(model["probability_resolution_bits"])
    probability_max = (1 << probability_bits) - 1

    lines = []
    lines.append("//! N-gram model for generating English-like words from entropy.")
    lines.append("//!")
    lines.append("//! This module is auto-generated by `models/generate_rust.py`")
    lines.append("//! Do not edit manually.")
    lines.append("")
    lines.append("/// Token vocabulary - maps token ID to token string.")
    lines.append("/// Tokens 0-255 are beginning tokens (^prefix).")
    lines.append("/// Tokens 256-511 are end tokens (suffix$).")
    lines.append("/// Tokens 512-1023 are middle tokens.")
    lines.append(f"pub const TOKENS: [&str; {len(id_to_token)}] = [")
    for token in id_to_token:
        escaped = escape_rust_string(token)
        lines.append(f'    "{escaped}",')
    lines.append("];")
    lines.append("")

    lines.append(f"pub const NGRAM_SIZE: usize = {ngram_size};")
    lines.append("pub const NONE_TOKEN: u16 = u16::MAX;")
    lines.append(f"pub const PROBABILITY_BITS: u8 = {probability_bits};")
    lines.append(f"pub const PROBABILITY_MAX: u32 = {probability_max};")
    lines.append("")

    def encode_token(token: int | None) -> int:
        return 65535 if token is None else int(token)

    def context_sort_key(value: tuple[int | None, ...]) -> tuple[int, ...]:
        return tuple(encode_token(item) for item in value)

    def build_transition_tables(
        raw_ngrams: list[list[object]],
        *,
        filter_fn=None,
    ) -> tuple[list[tuple[int | None, ...]], list[tuple[int, int]], list[tuple[int, int]]]:
        context_len = max(ngram_size - 1, 0)
        context_counts: dict[tuple[int | None, ...], list[tuple[int | None, int]]] = {}
        for ngram, weight in raw_ngrams:
            if len(ngram) != ngram_size:
                raise ValueError("ngram size does not match model ngram_size")
            context = tuple(ngram[:-1])
            next_token = ngram[-1]
            if filter_fn and not filter_fn(next_token):
                continue
            context_counts.setdefault(context, []).append((next_token, int(weight)))

        contexts = sorted(context_counts.keys(), key=context_sort_key)
        transition_index: list[tuple[int, int]] = []
        transition_data: list[tuple[int, int]] = []
        for context in contexts:
            start = len(transition_data)
            cumulative = 0
            for next_token, weight in context_counts[context]:
                cumulative += weight
                transition_data.append((encode_token(next_token), cumulative))
            length = len(transition_data) - start
            transition_index.append((start, length))
        return contexts, transition_index, transition_data

    def is_end_token(token: int | None) -> bool:
        if token is None:
            return False
        return 256 <= int(token) < 512

    context_len = max(ngram_size - 1, 0)
    lines.append(f"pub const CONTEXT_LEN: usize = {context_len};")

    middle_contexts, middle_index, middle_data = build_transition_tables(
        ngrams, filter_fn=lambda token: not is_end_token(token)
    )
    lines.append(
        f"pub const MIDDLE_CONTEXTS: [[u16; {context_len}]; {len(middle_contexts)}] = ["
    )
    for context in middle_contexts:
        encoded = [encode_token(item) for item in context]
        lines.append("    [" + ", ".join(str(item) for item in encoded) + "],")
    lines.append("];")
    lines.append("")

    lines.append("/// Index into `MIDDLE_TRANSITION_DATA`: (start, length) for each context.")
    lines.append(
        f"pub const MIDDLE_TRANSITION_INDEX: [(u32, u16); {len(middle_index)}] = ["
    )
    for start, length in middle_index:
        lines.append(f"    ({start}, {length}),")
    lines.append("];")
    lines.append("")

    lines.append("/// Middle transition data: (`next_token_id`, `cumulative_weight`)")
    lines.append(
        f"pub static MIDDLE_TRANSITION_DATA: [(u16, u32); {len(middle_data)}] = ["
    )
    for next_id, cumulative in middle_data:
        lines.append(f"    ({next_id}, {cumulative}),")
    lines.append("];")
    lines.append("")

    end_contexts, end_index, end_data = build_transition_tables(
        end_ngrams, filter_fn=is_end_token
    )
    lines.append(
        f"pub const END_CONTEXTS: [[u16; {context_len}]; {len(end_contexts)}] = ["
    )
    for context in end_contexts:
        encoded = [encode_token(item) for item in context]
        lines.append("    [" + ", ".join(str(item) for item in encoded) + "],")
    lines.append("];")
    lines.append("")

    lines.append("/// Index into `END_TRANSITION_DATA`: (start, length) for each context.")
    lines.append(
        f"pub const END_TRANSITION_INDEX: [(u32, u16); {len(end_index)}] = ["
    )
    for start, length in end_index:
        lines.append(f"    ({start}, {length}),")
    lines.append("];")
    lines.append("")

    lines.append("/// End transition data: (`end_token_id`, `cumulative_weight`)")
    lines.append(
        f"pub static END_TRANSITION_DATA: [(u16, u32); {len(end_data)}] = ["
    )
    for next_id, cumulative in end_data:
        lines.append(f"    ({next_id}, {cumulative}),")
    lines.append("];")
    lines.append("")

    return "\n".join(lines)


def main():
    parser = argparse.ArgumentParser(
        description="Generate Rust code from n-gram model"
    )
    parser.add_argument(
        "model",
        type=Path,
        help="Path to model JSON file",
    )
    parser.add_argument(
        "-o",
        "--output",
        type=Path,
        default=Path("../src/english_word_data.rs"),
        help="Output Rust file (default: ../src/english_word_data.rs)",
    )
    args = parser.parse_args()

    if not args.model.exists():
        print(f"Error: Model file {args.model} does not exist")
        return 1

    print(f"Loading model from {args.model}...")
    model = json.loads(args.model.read_text(encoding="utf-8"))

    print("Generating Rust code...")
    rust_code = generate_rust_code(model)

    print(f"Writing to {args.output}...")
    args.output.write_text(rust_code, encoding="utf-8")

    print(f"Generated {len(rust_code)} bytes of Rust code")
    print(f"  Tokens: {len(model['id_to_token'])}")
    print(f"  N-grams: {len(model['ngrams'])}")
    print(f"  End n-grams: {len(model['end_ngrams'])}")


if __name__ == "__main__":
    main()
