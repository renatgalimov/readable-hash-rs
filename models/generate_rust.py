#!/usr/bin/env python3
"""Generate Rust code from the n-gram model for word generation."""

import argparse
import json
from pathlib import Path


def escape_rust_string(s: str) -> str:
    """Escape a string for use in Rust code."""
    return s.replace("\\", "\\\\").replace('"', '\\"')


def generate_rust_code(model: dict) -> str:
    """Generate Rust module code from the model."""
    id_to_token = model["id_to_token"]
    begin_transitions = model["begin_transitions"]
    transitions = model["transitions"]
    end_transitions = model["end_transitions"]

    lines = []
    lines.append("//! N-gram model for generating English-like words from entropy.")
    lines.append("//!")
    lines.append("//! This module is auto-generated by models/generate_rust.py")
    lines.append("//! Do not edit manually.")
    lines.append("")
    lines.append("/// Token vocabulary - maps token ID to token string.")
    lines.append("/// Tokens 0-255 are beginning tokens (^prefix).")
    lines.append("/// Tokens 256-511 are end tokens (suffix$).")
    lines.append("/// Tokens 512-1023 are middle tokens.")
    lines.append(f"pub const TOKENS: [&str; {len(id_to_token)}] = [")
    for token in id_to_token:
        escaped = escape_rust_string(token)
        lines.append(f'    "{escaped}",')
    lines.append("];")
    lines.append("")

    # Begin transitions: [(token_id, cumulative_prob_u16), ...]
    # Convert float probabilities to u16 (0-65535) for efficiency
    lines.append("/// Beginning token transitions.")
    lines.append("/// Format: (token_id, cumulative_probability as u16)")
    lines.append(f"pub const BEGIN_TRANSITIONS: [(u16, u16); {len(begin_transitions)}] = [")
    for token_id, cumulative in begin_transitions:
        cum_u16 = min(65535, int(cumulative * 65535))
        lines.append(f"    ({token_id}, {cum_u16}),")
    lines.append("];")
    lines.append("")

    # Build transition arrays
    # We need a compact representation. Use a flat array with index pointers.
    # Format: TRANSITION_INDEX[token_id] = (start, len) into TRANSITION_DATA
    # TRANSITION_DATA = [(next_token_id, cumulative_u16), ...]

    transition_index = []
    transition_data = []
    for token_id in range(len(id_to_token)):
        key = str(token_id)
        if key in transitions:
            start = len(transition_data)
            for next_id, cumulative in transitions[key]:
                cum_u16 = min(65535, int(cumulative * 65535))
                transition_data.append((next_id, cum_u16))
            length = len(transition_data) - start
            transition_index.append((start, length))
        else:
            transition_index.append((0, 0))

    lines.append("/// Index into TRANSITION_DATA: (start, length) for each token.")
    lines.append(f"pub const TRANSITION_INDEX: [(u32, u16); {len(transition_index)}] = [")
    for start, length in transition_index:
        lines.append(f"    ({start}, {length}),")
    lines.append("];")
    lines.append("")

    lines.append("/// Transition data: (next_token_id, cumulative_probability as u16)")
    lines.append(f"pub const TRANSITION_DATA: [(u16, u16); {len(transition_data)}] = [")
    for next_id, cum_u16 in transition_data:
        lines.append(f"    ({next_id}, {cum_u16}),")
    lines.append("];")
    lines.append("")

    # End transitions - same format
    end_transition_index = []
    end_transition_data = []
    for token_id in range(len(id_to_token)):
        key = str(token_id)
        if key in end_transitions:
            start = len(end_transition_data)
            for next_id, cumulative in end_transitions[key]:
                cum_u16 = min(65535, int(cumulative * 65535))
                end_transition_data.append((next_id, cum_u16))
            length = len(end_transition_data) - start
            end_transition_index.append((start, length))
        else:
            end_transition_index.append((0, 0))

    lines.append("/// Index into END_TRANSITION_DATA: (start, length) for each token.")
    lines.append(f"pub const END_TRANSITION_INDEX: [(u32, u16); {len(end_transition_index)}] = [")
    for start, length in end_transition_index:
        lines.append(f"    ({start}, {length}),")
    lines.append("];")
    lines.append("")

    lines.append("/// End transition data: (end_token_id, cumulative_probability as u16)")
    lines.append(f"pub const END_TRANSITION_DATA: [(u16, u16); {len(end_transition_data)}] = [")
    for next_id, cum_u16 in end_transition_data:
        lines.append(f"    ({next_id}, {cum_u16}),")
    lines.append("];")
    lines.append("")

    # Add helper functions
    lines.append("""/// Find token by binary searching cumulative probabilities.
fn find_token(transitions: &[(u16, u16)], value: u16) -> u16 {
    for (token_id, cumulative) in transitions {
        if *cumulative >= value {
            return *token_id;
        }
    }
    transitions.last().map(|(id, _)| *id).unwrap_or(0)
}

/// Get the text for a token, stripping position markers.
fn token_text(token_id: u16) -> &'static str {
    let token = TOKENS[token_id as usize];
    let token = token.strip_prefix('^').unwrap_or(token);
    let token = token.strip_suffix('$').unwrap_or(token);
    token
}

/// Bit reader for consuming entropy from bytes.
struct BitReader<'a> {
    data: &'a [u8],
    bit_pos: usize,
}

impl<'a> BitReader<'a> {
    fn new(data: &'a [u8]) -> Self {
        Self { data, bit_pos: 0 }
    }

    fn read_u16(&mut self) -> u16 {
        let mut result: u16 = 0;
        for _ in 0..16 {
            let byte_idx = self.bit_pos / 8;
            let bit_idx = self.bit_pos % 8;
            if byte_idx < self.data.len() {
                let bit = (self.data[byte_idx] >> (7 - bit_idx)) & 1;
                result = (result << 1) | (bit as u16);
            }
            self.bit_pos += 1;
        }
        result
    }

    fn bits_remaining(&self) -> usize {
        self.data.len() * 8 - self.bit_pos
    }
}

/// Calculate target token count based on input byte length.
fn calculate_target_tokens(input_bytes: usize) -> usize {
    // Approximately 7 bits per token, minimum 2, maximum 16
    let estimated = (input_bytes * 8) / 7 + 1;
    estimated.clamp(2, 16)
}

/// Generate an English-like word from entropy bytes.
pub fn generate_word(entropy: &[u8]) -> String {
    let mut reader = BitReader::new(entropy);
    let target_tokens = calculate_target_tokens(entropy.len());
    let mut tokens: Vec<u16> = Vec::with_capacity(target_tokens);
    let mut result = String::new();

    // Select beginning token
    let begin_value = reader.read_u16();
    let first_token = find_token(&BEGIN_TRANSITIONS, begin_value);
    tokens.push(first_token);
    result.push_str(token_text(first_token));

    // Select middle tokens
    while tokens.len() < target_tokens - 1 && reader.bits_remaining() >= 8 {
        let current = *tokens.last().unwrap() as usize;
        let (start, len) = TRANSITION_INDEX[current];
        if len == 0 {
            break;
        }
        let trans = &TRANSITION_DATA[start as usize..(start as usize + len as usize)];
        let value = reader.read_u16();
        let next_token = find_token(trans, value);
        tokens.push(next_token);
        result.push_str(token_text(next_token));
    }

    // Select end token
    if let Some(&current) = tokens.last() {
        let (start, len) = END_TRANSITION_INDEX[current as usize];
        if len > 0 {
            let trans = &END_TRANSITION_DATA[start as usize..(start as usize + len as usize)];
            let value = if reader.bits_remaining() >= 16 {
                reader.read_u16()
            } else {
                0
            };
            let end_token = find_token(trans, value);
            tokens.push(end_token);
            result.push_str(token_text(end_token));
        }
    }

    result
}
""")

    return "\n".join(lines)


def main():
    parser = argparse.ArgumentParser(
        description="Generate Rust code from n-gram model"
    )
    parser.add_argument(
        "model",
        type=Path,
        help="Path to model JSON file",
    )
    parser.add_argument(
        "-o",
        "--output",
        type=Path,
        default=Path("../src/english_word.rs"),
        help="Output Rust file (default: ../src/english_word.rs)",
    )
    args = parser.parse_args()

    if not args.model.exists():
        print(f"Error: Model file {args.model} does not exist")
        return 1

    print(f"Loading model from {args.model}...")
    model = json.loads(args.model.read_text(encoding="utf-8"))

    print("Generating Rust code...")
    rust_code = generate_rust_code(model)

    print(f"Writing to {args.output}...")
    args.output.write_text(rust_code, encoding="utf-8")

    print(f"Generated {len(rust_code)} bytes of Rust code")
    print(f"  Tokens: {len(model['id_to_token'])}")
    print(f"  Begin transitions: {len(model['begin_transitions'])}")
    print(f"  Middle transitions: {len(model['transitions'])}")
    print(f"  End transitions: {len(model['end_transitions'])}")


if __name__ == "__main__":
    main()
