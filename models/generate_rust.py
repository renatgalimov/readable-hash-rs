#!/usr/bin/env python3
"""Generate Rust code from the n-gram model for word generation."""

import argparse
import json
from pathlib import Path


def escape_rust_string(s: str) -> str:
    """Escape a string for use in Rust code."""
    return s.replace("\\", "\\\\").replace('"', '\\"')


def generate_rust_code(model: dict) -> str:
    """Generate Rust module code from the model."""
    id_to_token = model["id_to_token"]
    begin_transitions = model["begin_transitions"]
    transitions = model["transitions"]
    end_transitions = model["end_transitions"]

    lines = []
    lines.append("//! N-gram model for generating English-like words from entropy.")
    lines.append("//!")
    lines.append("//! This module is auto-generated by models/generate_rust.py")
    lines.append("//! Do not edit manually.")
    lines.append("")
    lines.append("/// Token vocabulary - maps token ID to token string.")
    lines.append("/// Tokens 0-255 are beginning tokens (^prefix).")
    lines.append("/// Tokens 256-511 are end tokens (suffix$).")
    lines.append("/// Tokens 512-1023 are middle tokens.")
    lines.append(f"pub const TOKENS: [&str; {len(id_to_token)}] = [")
    for token in id_to_token:
        escaped = escape_rust_string(token)
        lines.append(f'    "{escaped}",')
    lines.append("];")
    lines.append("")

    # Begin transitions: [(token_id, cumulative_prob_u16), ...]
    # Convert float probabilities to u16 (0-65535) for efficiency
    lines.append("/// Beginning token transitions.")
    lines.append("/// Format: (token_id, cumulative_probability as u16)")
    lines.append(f"pub const BEGIN_TRANSITIONS: [(u16, u16); {len(begin_transitions)}] = [")
    for token_id, cumulative in begin_transitions:
        cum_u16 = min(65535, int(cumulative * 65535))
        lines.append(f"    ({token_id}, {cum_u16}),")
    lines.append("];")
    lines.append("")

    # Build transition arrays
    # We need a compact representation. Use a flat array with index pointers.
    # Format: TRANSITION_INDEX[token_id] = (start, len) into TRANSITION_DATA
    # TRANSITION_DATA = [(next_token_id, cumulative_u16), ...]

    transition_index = []
    transition_data = []
    for token_id in range(len(id_to_token)):
        key = str(token_id)
        if key in transitions:
            start = len(transition_data)
            for next_id, cumulative in transitions[key]:
                cum_u16 = min(65535, int(cumulative * 65535))
                transition_data.append((next_id, cum_u16))
            length = len(transition_data) - start
            transition_index.append((start, length))
        else:
            transition_index.append((0, 0))

    lines.append("/// Index into TRANSITION_DATA: (start, length) for each token.")
    lines.append(f"pub const TRANSITION_INDEX: [(u32, u16); {len(transition_index)}] = [")
    for start, length in transition_index:
        lines.append(f"    ({start}, {length}),")
    lines.append("];")
    lines.append("")

    lines.append("/// Transition data: (next_token_id, cumulative_probability as u16)")
    lines.append(f"pub const TRANSITION_DATA: [(u16, u16); {len(transition_data)}] = [")
    for next_id, cum_u16 in transition_data:
        lines.append(f"    ({next_id}, {cum_u16}),")
    lines.append("];")
    lines.append("")

    # End transitions - same format
    end_transition_index = []
    end_transition_data = []
    for token_id in range(len(id_to_token)):
        key = str(token_id)
        if key in end_transitions:
            start = len(end_transition_data)
            for next_id, cumulative in end_transitions[key]:
                cum_u16 = min(65535, int(cumulative * 65535))
                end_transition_data.append((next_id, cum_u16))
            length = len(end_transition_data) - start
            end_transition_index.append((start, length))
        else:
            end_transition_index.append((0, 0))

    lines.append("/// Index into END_TRANSITION_DATA: (start, length) for each token.")
    lines.append(f"pub const END_TRANSITION_INDEX: [(u32, u16); {len(end_transition_index)}] = [")
    for start, length in end_transition_index:
        lines.append(f"    ({start}, {length}),")
    lines.append("];")
    lines.append("")

    lines.append("/// End transition data: (end_token_id, cumulative_probability as u16)")
    lines.append(f"pub const END_TRANSITION_DATA: [(u16, u16); {len(end_transition_data)}] = [")
    for next_id, cum_u16 in end_transition_data:
        lines.append(f"    ({next_id}, {cum_u16}),")
    lines.append("];")
    lines.append("")

    # Add helper functions
    lines.append("""use crate::ByteReader;

/// Find token by binary searching cumulative probabilities.
fn find_token(transitions: &[(u16, u16)], value: u16) -> u16 {
    for (token_id, cumulative) in transitions {
        if *cumulative >= value {
            return *token_id;
        }
    }
    transitions.last().map(|(id, _)| *id).unwrap_or(0)
}

/// Get the text for a token, stripping position markers.
fn token_text(token_id: u16) -> &'static str {
    let token = TOKENS[token_id as usize];
    let token = token.strip_prefix('^').unwrap_or(token);
    let token = token.strip_suffix('$').unwrap_or(token);
    token
}

/// Bit reader that wraps a ByteReader, buffering bytes and reading bits.
struct BitReader<'a, R: ByteReader> {
    reader: &'a mut R,
    buffer: Vec<u8>,
    bit_pos: usize,
    exhausted: bool,
}

impl<'a, R: ByteReader> BitReader<'a, R> {
    fn new(reader: &'a mut R) -> Self {
        Self {
            reader,
            buffer: Vec::new(),
            bit_pos: 0,
            exhausted: false,
        }
    }

    /// Ensure we have at least `bits` available in the buffer.
    fn ensure_bits(&mut self, bits: usize) -> bool {
        if self.exhausted {
            return self.bits_available() >= bits;
        }

        let bytes_needed = (self.bit_pos + bits + 7) / 8;
        while self.buffer.len() < bytes_needed {
            let mut byte = [0u8; 1];
            if self.reader.read(&mut byte) == 0 {
                self.exhausted = true;
                break;
            }
            self.buffer.push(byte[0]);
        }
        self.bits_available() >= bits
    }

    fn bits_available(&self) -> usize {
        (self.buffer.len() * 8).saturating_sub(self.bit_pos)
    }

    fn read_u16(&mut self) -> Option<u16> {
        if !self.ensure_bits(16) {
            return None;
        }

        let mut result: u16 = 0;
        for _ in 0..16 {
            let byte_idx = self.bit_pos / 8;
            let bit_idx = self.bit_pos % 8;
            let bit = (self.buffer[byte_idx] >> (7 - bit_idx)) & 1;
            result = (result << 1) | (bit as u16);
            self.bit_pos += 1;
        }
        Some(result)
    }

    fn has_more(&mut self) -> bool {
        self.ensure_bits(8)
    }
}

/// Generate an English-like word from a ByteReader.
///
/// Reads bytes from the reader and generates tokens until the reader
/// is exhausted. The word consists of a beginning token, zero or more
/// middle tokens, and an end token.
pub fn generate_word<R: ByteReader>(reader: &mut R) -> String {
    let mut bit_reader = BitReader::new(reader);
    let mut tokens: Vec<u16> = Vec::new();
    let mut result = String::new();

    // Select beginning token
    let Some(begin_value) = bit_reader.read_u16() else {
        return String::new();
    };
    let first_token = find_token(&BEGIN_TRANSITIONS, begin_value);
    tokens.push(first_token);
    result.push_str(token_text(first_token));

    // Select middle tokens while we have entropy
    while bit_reader.has_more() {
        let current = *tokens.last().unwrap() as usize;
        let (start, len) = TRANSITION_INDEX[current];
        if len == 0 {
            break;
        }
        let Some(value) = bit_reader.read_u16() else {
            break;
        };
        let trans = &TRANSITION_DATA[start as usize..(start as usize + len as usize)];
        let next_token = find_token(trans, value);
        tokens.push(next_token);
        result.push_str(token_text(next_token));
    }

    // Select end token using remaining bits or default
    if let Some(&current) = tokens.last() {
        let (start, len) = END_TRANSITION_INDEX[current as usize];
        if len > 0 {
            let trans = &END_TRANSITION_DATA[start as usize..(start as usize + len as usize)];
            let value = bit_reader.read_u16().unwrap_or(0);
            let end_token = find_token(trans, value);
            tokens.push(end_token);
            result.push_str(token_text(end_token));
        }
    }

    result
}
""")

    return "\n".join(lines)


def main():
    parser = argparse.ArgumentParser(
        description="Generate Rust code from n-gram model"
    )
    parser.add_argument(
        "model",
        type=Path,
        help="Path to model JSON file",
    )
    parser.add_argument(
        "-o",
        "--output",
        type=Path,
        default=Path("../src/english_word.rs"),
        help="Output Rust file (default: ../src/english_word.rs)",
    )
    args = parser.parse_args()

    if not args.model.exists():
        print(f"Error: Model file {args.model} does not exist")
        return 1

    print(f"Loading model from {args.model}...")
    model = json.loads(args.model.read_text(encoding="utf-8"))

    print("Generating Rust code...")
    rust_code = generate_rust_code(model)

    print(f"Writing to {args.output}...")
    args.output.write_text(rust_code, encoding="utf-8")

    print(f"Generated {len(rust_code)} bytes of Rust code")
    print(f"  Tokens: {len(model['id_to_token'])}")
    print(f"  Begin transitions: {len(model['begin_transitions'])}")
    print(f"  Middle transitions: {len(model['transitions'])}")
    print(f"  End transitions: {len(model['end_transitions'])}")


if __name__ == "__main__":
    main()
